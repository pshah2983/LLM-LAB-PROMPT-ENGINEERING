{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Lab: Prompt Engineering Evaluation\n",
                "## Supply Chain Optimization Domain\n",
                "\n",
                "This notebook evaluates how prompt phrasing, structure, and constraints influence LLM response quality.\n",
                "\n",
                "**Team Roles:**\n",
                "- Prompt Architect\n",
                "- Evaluation Engineer\n",
                "- Safety & Mitigation Analyst\n",
                "- MLOps Integrator\n",
                "- Technical Communicator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install -r ../requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from src.llm_clients import LLMClient, load_config\n",
                "from src.prompts import PromptBuilder\n",
                "from src.evaluator import ResponseEvaluator, create_evaluation_summary\n",
                "from src.visualizations import generate_all_visualizations\n",
                "\n",
                "# Set your API key\n",
                "# Option 1: Set in environment\n",
                "# os.environ['GOOGLE_API_KEY'] = 'your-api-key-here'\n",
                "\n",
                "# Option 2: Already set via terminal export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load configuration\n",
                "config = load_config('../config/experiment_config.yaml')\n",
                "print(f\"Domain: {config['experiment']['domain']}\")\n",
                "print(f\"Model: {config['models']['primary']['name']}\")\n",
                "print(f\"Temperature: {config['models']['primary']['parameters']['temperature']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 1: Design Prompt Variants\n",
                "\n",
                "We test 5 distinct prompt designs:\n",
                "1. **P1_direct** - Naive, no constraints\n",
                "2. **P2_constrained** - Explicit format requirements\n",
                "3. **P3_role_based** - Expert persona\n",
                "4. **P4_reasoning_step** - Chain-of-Thought\n",
                "5. **P5_context_first** - Context before instruction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build all prompts\n",
                "prompt_builder = PromptBuilder('../config/experiment_config.yaml')\n",
                "prompts_table = prompt_builder.get_prompts_table()\n",
                "\n",
                "# Display prompt variants table\n",
                "df_prompts = pd.DataFrame(prompts_table)\n",
                "df_prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview a specific prompt (e.g., Chain-of-Thought)\n",
                "print(\"=\" * 60)\n",
                "print(\"P4 Reasoning Step (CoT) Prompt:\")\n",
                "print(\"=\" * 60)\n",
                "print(prompt_builder.build_prompt('P4_reasoning_step'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Experiments\n",
                "\n",
                "Execute each prompt variant and collect responses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM client\n",
                "client = LLMClient(config)\n",
                "print(f\"Using model: {client.get_model_info()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run all prompt variants\n",
                "responses = {}\n",
                "all_prompts = prompt_builder.build_all_prompts()\n",
                "\n",
                "for variant_id, data in all_prompts.items():\n",
                "    print(f\"Running {variant_id}: {data['name']}...\")\n",
                "    try:\n",
                "        result = client.generate(data['prompt'])\n",
                "        responses[variant_id] = {\n",
                "            'name': data['name'],\n",
                "            'description': data['description'],\n",
                "            'prompt': data['prompt'],\n",
                "            'response': result['response'],\n",
                "            'token_count': result['token_count'],\n",
                "            'latency_ms': result['latency_ms']\n",
                "        }\n",
                "        print(f\"  ✓ Completed ({result['token_count']} tokens, {result['latency_ms']}ms)\")\n",
                "    except Exception as e:\n",
                "        print(f\"  ✗ Error: {e}\")\n",
                "        responses[variant_id] = {'error': str(e)}\n",
                "\n",
                "print(f\"\\nCompleted {len([r for r in responses.values() if 'response' in r])}/{len(all_prompts)} variants\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View a sample response\n",
                "sample_variant = 'P1_direct'\n",
                "if sample_variant in responses and 'response' in responses[sample_variant]:\n",
                "    print(f\"Response from {sample_variant}:\")\n",
                "    print(\"=\" * 60)\n",
                "    print(responses[sample_variant]['response'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2: Evaluate Limitations and Mitigations\n",
                "\n",
                "Analyze responses for:\n",
                "- Factual hallucinations\n",
                "- Logical inconsistencies\n",
                "- Overconfidence\n",
                "- Missing key details\n",
                "- Over-elaboration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize evaluator\n",
                "evaluator = ResponseEvaluator('../config/experiment_config.yaml')\n",
                "\n",
                "# Evaluate all responses\n",
                "evaluations = {}\n",
                "\n",
                "for variant_id, data in responses.items():\n",
                "    if 'response' in data:\n",
                "        eval_result = evaluator.full_evaluation(\n",
                "            response=data['response'],\n",
                "            token_count=data['token_count']\n",
                "        )\n",
                "        evaluations[variant_id] = eval_result\n",
                "        \n",
                "        # Print summary\n",
                "        summary = eval_result['summary']\n",
                "        print(f\"{variant_id}: Accuracy={summary['accuracy_score']}/2, \"\n",
                "              f\"Completeness={summary['completeness_pct']}%, \"\n",
                "              f\"Issues={summary['issue_count']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed failure analysis for one variant\n",
                "analyze_variant = 'P1_direct'\n",
                "if analyze_variant in evaluations:\n",
                "    failures = evaluations[analyze_variant]['failure_behaviors']\n",
                "    print(f\"\\nFailure Analysis for {analyze_variant}:\")\n",
                "    print(\"-\" * 40)\n",
                "    for issue in failures['issues']:\n",
                "        print(f\"• [{issue['severity']}] {issue['type']}\")\n",
                "        print(f\"  {issue['description']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mitigation Strategies\n",
                "\n",
                "Based on observed failures, we can apply:\n",
                "1. **Chain-of-Thought (CoT)** - For reasoning improvement\n",
                "2. **Source Checking Requests** - \"Cite sources\" instruction\n",
                "3. **Confidence Calibration** - \"Express uncertainty where appropriate\"\n",
                "4. **Output Validation** - \"Verify your answer is complete\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Document mitigation observations\n",
                "mitigation_notes = \"\"\"\n",
                "## Observed Issues & Mitigations\n",
                "\n",
                "| Issue | Mitigation | Variant that helps |\n",
                "|-------|------------|-------------------|\n",
                "| Overconfidence | Add uncertainty language request | P4_reasoning_step |\n",
                "| Missing details | Explicit checklist in prompt | P2_constrained |\n",
                "| Over-elaboration | Word count limits | P2_constrained |\n",
                "| Hallucination | Request reasoning steps | P4_reasoning_step |\n",
                "\n",
                "### Key Insight:\n",
                "Add your team's observations here after running experiments.\n",
                "\"\"\"\n",
                "print(mitigation_notes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 3: Quantitative and Qualitative Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary table\n",
                "summary_rows = create_evaluation_summary(evaluations)\n",
                "df_summary = pd.DataFrame(summary_rows)\n",
                "df_summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add peer clarity ratings (manual input)\n",
                "# Team members should rate each response 1-5 on clarity\n",
                "\n",
                "# Example: Fill these in after peer review\n",
                "clarity_ratings = {\n",
                "    'P1_direct': 3,\n",
                "    'P2_constrained': 4,\n",
                "    'P3_role_based': 4,\n",
                "    'P4_reasoning_step': 5,\n",
                "    'P5_context_first': 4\n",
                "}\n",
                "\n",
                "# Update evaluations with clarity scores\n",
                "for variant_id, rating in clarity_ratings.items():\n",
                "    if variant_id in evaluations:\n",
                "        evaluations[variant_id]['clarity_score'] = rating"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate all visualizations\n",
                "import os\n",
                "os.makedirs('../results', exist_ok=True)\n",
                "\n",
                "figs = generate_all_visualizations(evaluations, '../results')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display accuracy comparison\n",
                "from src.visualizations import plot_accuracy_comparison\n",
                "plot_accuracy_comparison(evaluations)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display radar chart\n",
                "from src.visualizations import plot_radar_chart\n",
                "plot_radar_chart(evaluations)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Connection to Few-Shot and RAG\n",
                "\n",
                "**How these prompting techniques scale:**\n",
                "\n",
                "1. **Few-Shot Prompting**: Add examples to P2_constrained or P4_reasoning_step\n",
                "2. **Retrieval-Augmented Generation (RAG)**: P5_context_first naturally extends to RAG by\n",
                "   replacing static context with retrieved documents\n",
                "3. **Production Systems**: Use structured output (P2) + reasoning (P4) + dynamic context (P5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Few-shot extension of P2_constrained\n",
                "few_shot_prompt = \"\"\"\n",
                "Here is an example of a well-structured inventory optimization answer:\n",
                "\n",
                "Example Query: How to manage inventory for a retail store?\n",
                "Example Answer:\n",
                "1. Implement ABC analysis to prioritize high-value items\n",
                "2. Use EOQ formula: √(2DS/H) where D=demand, S=order cost, H=holding cost\n",
                "3. Set reorder point: Lead time demand + Safety stock\n",
                "4. Monitor with KPIs: turnover ratio, stockout rate\n",
                "5. Review quarterly and adjust for seasonality\n",
                "\n",
                "Now answer the following using the same structure:\n",
                "\n",
                "{query}\n",
                "\"\"\"\n",
                "\n",
                "print(\"Few-shot prompt template created.\")\n",
                "print(\"This extends P2_constrained with a concrete example.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save responses to JSON for reproducibility\n",
                "import json\n",
                "\n",
                "with open('../results/responses.json', 'w') as f:\n",
                "    json.dump(responses, f, indent=2)\n",
                "\n",
                "with open('../results/evaluations.json', 'w') as f:\n",
                "    # Convert to serializable format\n",
                "    serializable_evals = {k: v for k, v in evaluations.items()}\n",
                "    json.dump(serializable_evals, f, indent=2)\n",
                "\n",
                "print(\"Results saved to ../results/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary & Next Steps\n",
                "\n",
                "### Key Findings (Fill in after experiments):\n",
                "1. Best overall prompt variant: _____\n",
                "2. Most common failure mode: _____\n",
                "3. Most effective mitigation: _____\n",
                "\n",
                "### Recommendations:\n",
                "- For accuracy: Use _____\n",
                "- For efficiency: Use _____\n",
                "- For production: Combine _____ + _____"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}